---
title: Staging Ledger Technical Specification
status: final
owner: Nathan
version: 1.0.0-MPPP
date: 2025-09-27
spec_type: tech
master_prd_version: 2.3.0-MPPP
roadmap_version: 2.0.0-MPPP
---

# Staging Ledger ‚Äî Technical Specification

## 1. Scope & Interfaces

### 1.1 Module Boundary

**Package:** `@adhd-brain/staging-ledger`

**Responsibility:** Durable capture staging with content-hash deduplication, crash recovery, and audit transparency

**Public APIs:**

```typescript
// Core capture operations
export interface StagingLedger {
  insertCapture(input: CaptureInsert): Promise<CaptureInsertResult>;
  updateTranscription(capture_id: string, update: TranscriptionUpdate): Promise<void>;
  markTranscriptionFailed(capture_id: string, error: string): Promise<void>;
  recordExport(capture_id: string, audit: ExportAudit): Promise<void>;

  // Query operations
  checkDuplicate(content_hash: string): Promise<DuplicateCheckResult>;
  queryPendingExports(): Promise<Capture[]>;
  queryRecoverable(): Promise<Capture[]>;

  // Health operations
  getHealthStatus(): Promise<HealthStatus>;

  // Maintenance operations
  createBackup(): Promise<BackupResult>;
  verifyBackup(backup_path: string): Promise<VerificationResult>;
  pruneExported(days: number): Promise<PruneResult>;
}
```

**Internal Modules (Not Exported):**

- `db-connection.ts` - SQLite connection pool
- `schema-migrations.ts` - Schema versioning and migrations
- `hash-utils.ts` - SHA-256 normalization and computation
- `state-machine.ts` - Status transition validation
- `metrics.ts` - Local NDJSON metrics emission

### 1.2 TypeScript Interfaces

```typescript
// === Core Types ===

export interface CaptureInsert {
  id: string;                      // ULID (pre-generated by caller)
  source: 'voice' | 'email';
  raw_content: string;
  content_hash?: string;           // Optional until available (voice)
  meta_json: CaptureMetadata;
}

export interface CaptureMetadata {
  channel: 'voice' | 'email';
  channel_native_id: string;       // file_path or message_id

  // Voice-specific
  audio_fp?: string;               // SHA-256 of first 4MB
  file_path?: string;              // Apple Voice Memos path
  duration_ms?: number;

  // Email-specific
  message_id?: string;             // Gmail Message-ID
  from?: string;
  subject?: string;
  received_at?: Date;
}

export interface CaptureInsertResult {
  success: boolean;
  capture_id: string;
  is_duplicate: boolean;           // Layer 1 duplicate detected
  error?: string;
}

export interface TranscriptionUpdate {
  transcript_text: string;
  content_hash: string;            // SHA-256 of normalized transcript
}

export interface ExportAudit {
  vault_path: string;              // inbox/<ulid>.md
  hash_at_export: string | null;  // NULL for placeholder
  mode: 'initial' | 'duplicate_skip' | 'placeholder';
  error_flag: boolean;
}

export interface Capture {
  id: string;
  source: 'voice' | 'email';
  raw_content: string;
  content_hash: string | null;
  status: CaptureStatus;
  meta_json: CaptureMetadata;
  created_at: Date;
  updated_at: Date;
}

export type CaptureStatus =
  | 'staged'
  | 'transcribed'
  | 'failed_transcription'
  | 'exported'
  | 'exported_duplicate'
  | 'exported_placeholder';

export interface DuplicateCheckResult {
  is_duplicate: boolean;
  existing_capture_id?: string;
}

export interface HealthStatus {
  queue_depth: number;             // Pending captures
  last_backup: Date | null;
  errors_24h: Array<{ stage: string; count: number }>;
  placeholder_ratio_7d: number;    // Percentage
  database_size_mb: number;
}

export interface BackupResult {
  success: boolean;
  backup_path: string;
  size_bytes: number;
  duration_ms: number;
}

export interface VerificationResult {
  success: boolean;
  integrity_check_passed: boolean;
  hash_match: boolean;
  error?: string;
}

export interface PruneResult {
  rows_deleted: number;
  duration_ms: number;
}

// === Error Types ===

export class StagingLedgerError extends Error {
  constructor(message: string, public readonly code: string) {
    super(message);
    this.name = 'StagingLedgerError';
  }
}

export class DuplicateConstraintError extends StagingLedgerError {
  constructor(channel: string, native_id: string) {
    super(
      `Duplicate capture: ${channel}/${native_id}`,
      'DUPLICATE_CONSTRAINT'
    );
  }
}

export class InvalidStateTransitionError extends StagingLedgerError {
  constructor(current: CaptureStatus, next: CaptureStatus) {
    super(
      `Invalid state transition: ${current} ‚Üí ${next}`,
      'INVALID_TRANSITION'
    );
  }
}

export class DatabaseCorruptionError extends StagingLedgerError {
  constructor(message: string) {
    super(message, 'DATABASE_CORRUPTION');
  }
}
```

### 1.3 CLI Integration

**Command:** `capture staging [subcommand]`

**Subcommands:**

```bash
# Health check
capture staging health
# ‚Üí Displays health status from getHealthStatus()

# Manual backup
capture staging backup
# ‚Üí Calls createBackup()

# Verify backup
capture staging verify <backup-path>
# ‚Üí Calls verifyBackup(backup_path)

# Prune old captures
capture staging prune --days 90
# ‚Üí Calls pruneExported(90)

# Show pending captures
capture staging pending
# ‚Üí Calls queryPendingExports(), displays table
```

---

## 2. Data & Storage

### 2.1 SQLite Schema (Complete)

**MPPP Constraint:** Single `content_hash` column using SHA-256 only. No dual-hash columns (BLAKE3 deferred to Phase 2+ per ADR-0002).

```sql
-- ==================================================================
-- Schema Version: 1
-- Description: Initial MPPP schema with 4 tables (SHA-256 only)
-- ==================================================================

-- === Table 1: captures (Ephemeral Staging) ===

CREATE TABLE IF NOT EXISTS captures (
    id TEXT PRIMARY KEY,                     -- ULID (time-orderable)
    source TEXT NOT NULL CHECK (source IN ('voice', 'email')),
    raw_content TEXT NOT NULL,
    content_hash TEXT,                       -- SHA-256 hex (64 chars, nullable until transcription)
    status TEXT NOT NULL CHECK (status IN (
        'staged',
        'transcribed',
        'failed_transcription',
        'exported',
        'exported_duplicate',
        'exported_placeholder'
    )),
    meta_json TEXT NOT NULL,                 -- JSON string
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Unique content hash (NULL values ignored by SQLite)
CREATE UNIQUE INDEX IF NOT EXISTS captures_content_hash_idx
    ON captures(content_hash);

-- Prevent duplicate staging of same physical/logical item
CREATE UNIQUE INDEX IF NOT EXISTS captures_channel_native_uid
    ON captures(
        json_extract(meta_json, '$.channel'),
        json_extract(meta_json, '$.channel_native_id')
    );

-- Fast status filtering for pending exports
CREATE INDEX IF NOT EXISTS captures_status_idx
    ON captures(status);

-- Fast created_at ordering for recovery query
CREATE INDEX IF NOT EXISTS captures_created_at_idx
    ON captures(created_at);

-- === Table 2: exports_audit (Immutable Trail) ===

CREATE TABLE IF NOT EXISTS exports_audit (
    id TEXT PRIMARY KEY,                     -- ULID
    capture_id TEXT NOT NULL,
    vault_path TEXT NOT NULL,                -- inbox/<ulid>.md
    hash_at_export TEXT,                     -- SHA-256 or NULL for placeholder
    exported_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    mode TEXT NOT NULL CHECK (mode IN (
        'initial',
        'duplicate_skip',
        'placeholder'
    )),
    error_flag INTEGER DEFAULT 0 CHECK (error_flag IN (0, 1)),

    FOREIGN KEY (capture_id) REFERENCES captures(id)
        ON DELETE CASCADE
);

-- Fast lookup by capture_id
CREATE INDEX IF NOT EXISTS exports_audit_capture_idx
    ON exports_audit(capture_id);

-- === Table 3: errors_log (Diagnostics) ===

CREATE TABLE IF NOT EXISTS errors_log (
    id TEXT PRIMARY KEY,                     -- ULID
    capture_id TEXT,                         -- Nullable for system-level errors
    stage TEXT NOT NULL CHECK (stage IN (
        'poll',
        'transcribe',
        'export',
        'backup',
        'integrity'
    )),
    message TEXT NOT NULL,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,

    FOREIGN KEY (capture_id) REFERENCES captures(id)
        ON DELETE SET NULL
);

-- Fast grouping by stage
CREATE INDEX IF NOT EXISTS errors_log_stage_idx
    ON errors_log(stage);

-- Fast time-based queries (last 24h)
CREATE INDEX IF NOT EXISTS errors_log_created_at_idx
    ON errors_log(created_at);

-- === Table 4: sync_state (Cursors) ===

CREATE TABLE IF NOT EXISTS sync_state (
    key TEXT PRIMARY KEY,                    -- e.g. 'gmail_history_id'
    value TEXT NOT NULL,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- No indexes needed (< 100 rows, PK is clustered)
```

### 2.2 SQLite PRAGMAs

```sql
-- === Connection Initialization ===

PRAGMA journal_mode = WAL;                   -- Write-ahead logging
PRAGMA synchronous = NORMAL;                 -- Balance safety/performance
PRAGMA foreign_keys = ON;                    -- Referential integrity
PRAGMA busy_timeout = 5000;                  -- Per [Database Resilience](../../guides/guide-resilience-patterns.md#database-resilience)
PRAGMA wal_autocheckpoint = 1000;            -- Checkpoint every 1000 pages
PRAGMA cache_size = -2000;                   -- 2MB cache
PRAGMA temp_store = MEMORY;                  -- Temp tables in RAM
```

**Rationale:**

- **WAL mode:** Concurrent reads, crash-safe, faster writes
- **NORMAL sync:** Acceptable risk for local-only ledger (< 1s data loss window on OS crash)
- **Foreign keys:** Prevent orphaned audit rows
- **Busy timeout:** Handle burst captures per [Database Pattern](../../guides/guide-resilience-patterns.md#sqlite-pragmas)
- **Cache size:** Small cache (2MB) sufficient for MPPP scope

### 2.3 Schema Migrations

**Migration Strategy:**

- Append-only numbered migrations: `/migrations/0001_init.sql`
- Schema version stored in `sync_state`: `('schema_version', '1')`
- Migrations run on connection initialization

**Migration Example:**

```typescript
// migrations/0001_init.sql
-- Schema version 1
-- Created: 2025-09-27

-- (Full schema from section 2.1)

-- Record schema version
INSERT INTO sync_state (key, value)
VALUES ('schema_version', '1')
ON CONFLICT (key) DO UPDATE SET value = '1';
```

**Future Migrations:**

```typescript
// migrations/0002_add_blake3_hash.sql (example)
-- Schema version 2
-- Add BLAKE3 dual-hash support per ADR-0002

ALTER TABLE captures ADD COLUMN content_hash_blake3 TEXT;

CREATE INDEX captures_blake3_hash_idx ON captures(content_hash_blake3);

UPDATE sync_state SET value = '2' WHERE key = 'schema_version';
```

### 2.4 Database File Location

**Path:** `${VAULT_ROOT}/.adhd-brain/ledger.sqlite`

**Rationale:**

- Co-located with vault (same filesystem guarantees)
- Hidden folder (`.adhd-brain`) avoids Obsidian indexing
- Portable with vault (manual backup includes ledger)

**Backup Path:** `${VAULT_ROOT}/.adhd-brain/.backups/ledger-YYYYMMDD-HH.sqlite`

---

## 3. Control Flow

### 3.1 Capture Insert Flow

```typescript
async function insertCapture(input: CaptureInsert): Promise<CaptureInsertResult> {
  const startTime = performance.now();

  try {
    // 1. Validate input
    if (!input.id || !isULID(input.id)) {
      throw new StagingLedgerError('Invalid ULID', 'INVALID_INPUT');
    }

    // 2. Normalize content if hash provided
    let normalizedHash = input.content_hash;
    if (normalizedHash) {
      const normalized = normalizeText(input.raw_content);
      normalizedHash = sha256(normalized);
    }

    // 3. Prepare statement
    const stmt = db.prepare(`
      INSERT INTO captures (id, source, raw_content, content_hash, status, meta_json)
      VALUES (?, ?, ?, ?, 'staged', ?)
    `);

    // 4. Execute insert (Layer 1 duplicate detection via UNIQUE constraint)
    await stmt.run(
      input.id,
      input.source,
      input.raw_content,
      normalizedHash,
      JSON.stringify(input.meta_json)
    );

    // 5. Emit metrics
    const duration = performance.now() - startTime;
    emitMetric({
      name: 'capture_staging_ms',
      value: duration,
      labels: { source: input.source }
    });

    emitMetric({
      name: 'captures_inserted_total',
      value: 1,
      labels: { source: input.source }
    });

    return {
      success: true,
      capture_id: input.id,
      is_duplicate: false
    };

  } catch (error) {
    // Layer 1 duplicate detected (UNIQUE constraint violation)
    if (error.code === 'SQLITE_CONSTRAINT_UNIQUE') {
      return {
        success: true,
        capture_id: input.id,
        is_duplicate: true
      };
    }

    // Log error
    await logError({
      capture_id: null,
      stage: 'poll',
      message: error.message
    });

    throw error;
  }
}
```

### 3.2 Transcription Update Flow

```typescript
async function updateTranscription(
  capture_id: string,
  update: TranscriptionUpdate
): Promise<void> {
  const db = getConnection();

  await db.transaction(async (tx) => {
    // 1. Fetch current capture
    const capture = await tx.get<Capture>(
      'SELECT * FROM captures WHERE id = ?',
      [capture_id]
    );

    if (!capture) {
      throw new StagingLedgerError('Capture not found', 'NOT_FOUND');
    }

    // 2. Validate state transition (staged ‚Üí transcribed)
    validateTransition(capture.status, 'transcribed');

    // 3. Validate hash mutates at most once
    if (capture.content_hash && capture.content_hash !== update.content_hash) {
      throw new StagingLedgerError(
        'Hash already set, cannot update',
        'IMMUTABLE_HASH'
      );
    }

    // 4. Update capture
    await tx.run(
      `UPDATE captures
       SET raw_content = ?,
           content_hash = ?,
           status = 'transcribed',
           updated_at = CURRENT_TIMESTAMP
       WHERE id = ?`,
      [update.transcript_text, update.content_hash, capture_id]
    );

    // 5. Emit metric
    emitMetric({
      name: 'transcription_complete_total',
      value: 1
    });
  });
}
```

### 3.3 Duplicate Check Flow

```typescript
async function checkDuplicate(content_hash: string): Promise<DuplicateCheckResult> {
  const startTime = performance.now();

  try {
    // Query for existing hash
    const result = await db.get<{ id: string }>(
      'SELECT id FROM captures WHERE content_hash = ? LIMIT 1',
      [content_hash]
    );

    // Emit metric
    const duration = performance.now() - startTime;
    emitMetric({
      name: 'dedup_check_ms',
      value: duration
    });

    if (result) {
      emitMetric({
        name: 'dedup_hits_total',
        value: 1,
        labels: { layer: 'content_hash' }
      });

      return {
        is_duplicate: true,
        existing_capture_id: result.id
      };
    }

    return { is_duplicate: false };

  } catch (error) {
    await logError({
      capture_id: null,
      stage: 'export',
      message: `Duplicate check failed: ${error.message}`
    });
    throw error;
  }
}
```

### 3.4 Export Recording Flow

```typescript
async function recordExport(
  capture_id: string,
  audit: ExportAudit
): Promise<void> {
  const db = getConnection();

  await db.transaction(async (tx) => {
    // 1. Fetch current capture
    const capture = await tx.get<Capture>(
      'SELECT * FROM captures WHERE id = ?',
      [capture_id]
    );

    if (!capture) {
      throw new StagingLedgerError('Capture not found', 'NOT_FOUND');
    }

    // 2. Determine next status
    let nextStatus: CaptureStatus;
    if (audit.mode === 'duplicate_skip') {
      nextStatus = 'exported_duplicate';
    } else if (audit.mode === 'placeholder') {
      nextStatus = 'exported_placeholder';
    } else {
      nextStatus = 'exported';
    }

    // 3. Validate state transition
    validateTransition(capture.status, nextStatus);

    // 4. Insert audit record
    const auditId = ulid();
    await tx.run(
      `INSERT INTO exports_audit (id, capture_id, vault_path, hash_at_export, mode, error_flag)
       VALUES (?, ?, ?, ?, ?, ?)`,
      [
        auditId,
        capture_id,
        audit.vault_path,
        audit.hash_at_export,
        audit.mode,
        audit.error_flag ? 1 : 0
      ]
    );

    // 5. Update capture status (terminal state, immutable)
    await tx.run(
      `UPDATE captures
       SET status = ?,
           updated_at = CURRENT_TIMESTAMP
       WHERE id = ?`,
      [nextStatus, capture_id]
    );

    // 6. Emit metrics
    emitMetric({
      name: 'captures_exported_total',
      value: 1,
      labels: { mode: audit.mode }
    });

    if (audit.mode === 'placeholder') {
      emitMetric({
        name: 'placeholder_exports_total',
        value: 1
      });
    }
  });
}
```

### 3.5 Crash Recovery Flow

**Recovery Scan Algorithm:**

- **Ordering:** `created_at ASC` (process oldest captures first to preserve FIFO order)
- **Batching:** Single query returns all recoverable captures (MPPP scope < 1000 expected)
- **Filtering:** `status IN ('staged', 'transcribed', 'failed_transcription')` (excludes exported captures)
- **Processing:** Sequential (no parallel recovery to simplify state machine consistency)

**Trigger:** Recovery scan runs once on application startup before normal polling begins.

**Implementation:**

```typescript
async function queryRecoverable(): Promise<Capture[]> {
  const startTime = performance.now();

  try {
    // Single query returns all recoverable captures in chronological order
    const captures = await db.all<Capture[]>(
      `SELECT * FROM captures
       WHERE status IN ('staged', 'transcribed', 'failed_transcription')
       ORDER BY created_at ASC`
    );

    const duration = performance.now() - startTime;
    emitMetric({
      name: 'recovery_query_ms',
      value: duration
    });

    emitMetric({
      name: 'recovery_captures_found',
      value: captures.length
    });

    return captures;

  } catch (error) {
    await logError({
      capture_id: null,
      stage: 'poll',
      message: `Recovery query failed: ${error.message}`
    });
    throw error;
  }
}
```

**Batching Strategy (Phase 2+):** If recovery query exceeds 2s p95 (> 1000 items), introduce pagination with `LIMIT 100 OFFSET N`. Trigger: Volume exceeds MPPP assumptions.

### 3.6 State Machine Validation

```typescript
function validateTransition(current: CaptureStatus, next: CaptureStatus): void {
  // Terminal states cannot transition
  if (current.startsWith('exported')) {
    throw new InvalidStateTransitionError(current, next);
  }

  // Staged can go to transcribed, failed, or duplicate
  if (current === 'staged') {
    const allowed = ['transcribed', 'failed_transcription', 'exported_duplicate'];
    if (!allowed.includes(next)) {
      throw new InvalidStateTransitionError(current, next);
    }
    return;
  }

  // Transcribed can only go to exported states
  if (current === 'transcribed') {
    const allowed = ['exported', 'exported_duplicate'];
    if (!allowed.includes(next)) {
      throw new InvalidStateTransitionError(current, next);
    }
    return;
  }

  // Failed transcription can only go to placeholder export
  if (current === 'failed_transcription') {
    if (next !== 'exported_placeholder') {
      throw new InvalidStateTransitionError(current, next);
    }
    return;
  }

  throw new InvalidStateTransitionError(current, next);
}
```

---

## 4. TDD Applicability Decision

**Per TDD Applicability Guide v1.0.0 (Section 2-3)**

### Risk Classification: HIGH

**Rationale:**

- üíæ **Storage integrity:** SQLite transactions, foreign keys, unique constraints
- üîÅ **Concurrency safety:** WAL mode, atomic inserts, status machine
- üß† **Core deduplication:** SHA-256 normalization, duplicate detection
- üîê **Data durability:** Zero data loss on crash (commits before async work)

### Decision: TDD Required (Test-Driven Development Mandatory)

**Why:**

Every staging ledger component touches data integrity, deduplication, or crash recovery‚Äîall HIGH-risk areas. TDD is required to prove correctness before implementation.

### Scope Under TDD

#### Unit Tests (Required)

**Pure Logic:**

- ‚úÖ SHA-256 hash normalization (deterministic output)
- ‚úÖ Text normalization (whitespace, line endings)
- ‚úÖ Status state machine transitions (validate all paths)
- ‚úÖ ULID validation and generation
- ‚úÖ Hash collision handling (theoretical edge case)
- ‚úÖ JSON metadata serialization/deserialization

**Test Example:**

```typescript
describe('Hash Normalization', () => {
  it('produces deterministic hash for identical content', () => {
    const text1 = '  Hello World\n\n';
    const text2 = 'Hello World\n';

    const hash1 = computeContentHash(text1);
    const hash2 = computeContentHash(text2);

    expect(hash1).toBe(hash2);
  });

  it('produces different hash for different content', () => {
    const text1 = 'Hello World';
    const text2 = 'Goodbye World';

    const hash1 = computeContentHash(text1);
    const hash2 = computeContentHash(text2);

    expect(hash1).not.toBe(hash2);
  });
});
```

#### Integration Tests (Required)

**Full Data Flow:**

- ‚úÖ Capture insert ‚Üí duplicate check ‚Üí export (happy path)
- ‚úÖ Voice capture ‚Üí transcription update ‚Üí export (late hash binding)
- ‚úÖ Email capture ‚Üí duplicate skip (Layer 2 dedup)
- ‚úÖ Transcription failure ‚Üí placeholder export
- ‚úÖ Crash recovery (staged ‚Üí resume transcription)
- ‚úÖ Crash recovery (transcribed ‚Üí resume export)
- ‚úÖ Foreign key cascade (delete capture ‚Üí audit remains)
- ‚úÖ Unique constraint enforcement (channel + native_id)

**Test Example:**

```typescript
describe('Voice Capture Flow (Integration)', () => {
  it('stages ‚Üí transcribes ‚Üí exports without duplication', async () => {
    const ledger = createTestLedger(); // In-memory SQLite

    // 1. Insert capture
    const result = await ledger.insertCapture({
      id: ulid(),
      source: 'voice',
      raw_content: '',
      meta_json: {
        channel: 'voice',
        channel_native_id: '/path/to/memo.m4a',
        audio_fp: 'sha256...'
      }
    });

    expect(result.success).toBe(true);

    // 2. Update transcription
    await ledger.updateTranscription(result.capture_id, {
      transcript_text: 'Hello world',
      content_hash: sha256('Hello world')
    });

    // 3. Check duplicate (should be unique)
    const dupCheck = await ledger.checkDuplicate(sha256('Hello world'));
    expect(dupCheck.is_duplicate).toBe(false);

    // 4. Record export
    await ledger.recordExport(result.capture_id, {
      vault_path: `inbox/${result.capture_id}.md`,
      hash_at_export: sha256('Hello world'),
      mode: 'initial',
      error_flag: false
    });

    // 5. Verify terminal state
    const capture = await ledger.getCapture(result.capture_id);
    expect(capture.status).toBe('exported');

    // 6. Verify audit trail
    const audits = await ledger.getExportAudits(result.capture_id);
    expect(audits).toHaveLength(1);
    expect(audits[0].mode).toBe('initial');
  });
});
```

#### Contract Tests (Required)

**API Boundaries:**

- ‚úÖ CaptureInsert interface validation
- ‚úÖ TranscriptionUpdate interface validation
- ‚úÖ ExportAudit interface validation
- ‚úÖ Error types thrown correctly
- ‚úÖ Metrics emitted in expected format

**Test Example:**

```typescript
describe('API Contract: insertCapture', () => {
  it('rejects invalid ULID', async () => {
    const ledger = createTestLedger();

    await expect(
      ledger.insertCapture({
        id: 'not-a-ulid',
        source: 'voice',
        raw_content: '',
        meta_json: { channel: 'voice', channel_native_id: 'test' }
      })
    ).rejects.toThrow(StagingLedgerError);
  });

  it('returns duplicate flag on constraint violation', async () => {
    const ledger = createTestLedger();

    const input = {
      id: ulid(),
      source: 'email' as const,
      raw_content: 'test',
      content_hash: sha256('test'),
      meta_json: {
        channel: 'email' as const,
        channel_native_id: 'message-123'
      }
    };

    // First insert succeeds
    const result1 = await ledger.insertCapture(input);
    expect(result1.is_duplicate).toBe(false);

    // Second insert detects duplicate
    const result2 = await ledger.insertCapture(input);
    expect(result2.is_duplicate).toBe(true);
  });
});
```

#### Fault Injection Tests (Phase 2)

**Crash Simulation:**

- ‚úÖ Crash after capture insert (verify staged row persists)
- ‚úÖ Crash after transcription (verify transcribed row persists)
- ‚úÖ Crash before export (verify no vault file, recoverable)
- ‚úÖ Crash after temp file write (verify cleanup + retry per [Recovery Pattern](../../guides/guide-resilience-patterns.md#crash-recovery))
- ‚úÖ Crash after audit insert (verify status eventually updated)

**Test Example (using TestKit fault injection):**

```typescript
describe('Crash Recovery (Fault Injection)', () => {
  it('recovers from crash after transcription', async () => {
    const ledger = createTestLedger();

    // 1. Insert and transcribe
    const captureId = ulid();
    await ledger.insertCapture({ id: captureId, ... });
    await ledger.updateTranscription(captureId, { ... });

    // 2. Simulate crash (kill connection)
    ledger.close();

    // 3. Restart ledger
    const ledger2 = createTestLedger(sameDatabasePath);

    // 4. Query recoverable captures
    const recoverable = await ledger2.queryRecoverable();
    expect(recoverable).toHaveLength(1);
    expect(recoverable[0].id).toBe(captureId);
    expect(recoverable[0].status).toBe('transcribed');
  });
});
```

### Out-of-Scope (YAGNI Deferrals)

**Not Testing Now:**

- ‚ùå Performance benchmarks (defer to Phase 2+ if p95 > 11s)
- ‚ùå Load testing (single user assumption, < 200 captures/day)
- ‚ùå Concurrent write stress (sequential processing only)
- ‚ùå Multi-device sync conflicts (out of scope)
- ‚ùå Advanced query optimization (no regression observed yet)
- ‚ùå BLAKE3 dual-hash migration (ADR-0002 deferred)

### Trigger to Revisit TDD Scope

| Condition | Action |
|-----------|--------|
| Introduce concurrency | Add race condition tests with parallel inserts |
| Add BLAKE3 dual-hash (ADR-0002) | Test hash migration path, backfill correctness |
| Database size > 500MB | Add performance regression suite |
| False duplicate incident | Enhance hash collision edge case testing |
| Query p95 > 11s | Profile and add composite index performance tests |

---

## 5. Dependencies & Contracts

### 5.1 Upstream Dependencies

**External Libraries:**

```json
{
  "better-sqlite3": "^9.0.0",      // SQLite driver (synchronous, fast)
  "ulid": "^2.3.0",                // ULID generation
  "crypto": "built-in"             // Node.js crypto (SHA-256)
}
```

**Internal Packages:**

- `@adhd-brain/shared-config` - TypeScript, ESLint configs
- `@adhd-brain/testkit` - Test fixtures, in-memory DB, mocks

### 5.2 Downstream Consumers

**Voice Capture Package:**

```typescript
import { StagingLedger } from '@adhd-brain/staging-ledger';

const ledger = new StagingLedger(vaultPath);

// Insert voice capture
await ledger.insertCapture({
  id: ulid(),
  source: 'voice',
  raw_content: '',
  meta_json: {
    channel: 'voice',
    channel_native_id: filePath,
    audio_fp: audioFingerprint
  }
});
```

**Email Capture Package:**

```typescript
import { StagingLedger } from '@adhd-brain/staging-ledger';

const ledger = new StagingLedger(vaultPath);

// Insert email capture with hash
await ledger.insertCapture({
  id: ulid(),
  source: 'email',
  raw_content: bodyText,
  content_hash: sha256(normalizeText(bodyText)),
  meta_json: {
    channel: 'email',
    channel_native_id: messageId,
    message_id: messageId,
    from: fromAddress,
    subject: emailSubject
  }
});
```

**Transcription Worker:**

```typescript
import { StagingLedger } from '@adhd-brain/staging-ledger';

const ledger = new StagingLedger(vaultPath);

// Update after successful transcription
await ledger.updateTranscription(captureId, {
  transcript_text: whisperOutput,
  content_hash: sha256(normalizeText(whisperOutput))
});

// Mark failure
// Timeout per [Whisper Pattern](../../guides/guide-resilience-patterns.md#whisper-transcription)
await ledger.markTranscriptionFailed(captureId, 'Whisper timeout exceeded');
```

**Export Worker:**

```typescript
import { StagingLedger } from '@adhd-brain/staging-ledger';

const ledger = new StagingLedger(vaultPath);

// Record successful export
await ledger.recordExport(captureId, {
  vault_path: `inbox/${captureId}.md`,
  hash_at_export: capture.content_hash,
  mode: 'initial',
  error_flag: false
});
```

### 5.3 External Adapters

**SQLite Driver:**

- Synchronous API (better-sqlite3) for simplicity
- WAL mode support
- Transaction support
- Foreign key support

**File System:**

- Database file: `.adhd-brain/ledger.sqlite`
- Backup directory: `.adhd-brain/.backups/`
- Metrics directory: `.adhd-brain/.metrics/`

---

## 6. Risks & Mitigations

### 6.1 High-Priority Risks

| Risk | Impact | Mitigation | Verification |
|------|--------|------------|--------------|
| Data loss (crash during capture) | Critical | WAL mode + atomic insert before async work | Fault injection tests |
| SQLite corruption | Critical | Hourly backups + verification + integrity checks | Weekly restore test |
| Duplicate exports (dedup failure) | High anxiety | SHA-256 + UNIQUE constraint + duplicate check | Integration tests |
| Backup verification drift | High | Consecutive failure escalation policy | Hourly verification |
| Invalid state transitions | High | validateTransition() + unit tests | State machine tests |
| Hash collision (theoretical) | Low | Accept risk (SHA-256 = 2^256 space) | Document edge case |

### 6.2 Medium-Priority Risks

| Risk | Impact | Mitigation | Verification |
|------|--------|------------|--------------|
| Performance degradation (> 10k captures) | Medium | Monitor p95, add indexes if needed | Performance metrics |
| Storage runaway growth | Medium | 90-day retention + size monitoring | Health command warns at 100MB |
| Foreign key orphans | Low | FOREIGN KEY constraints + ON DELETE CASCADE | Contract tests |
| Metrics disk usage | Low | Daily rotation + 30-day retention | Cleanup job |

### 6.3 Deferred Risks (Future Phases)

| Risk | Defer Reason | Revisit Trigger |
|------|--------------|-----------------|
| BLAKE3 migration complexity | SHA-256 sufficient for MPPP | > 200 daily captures OR false duplicate |
| Concurrent write conflicts | Sequential processing only | Backlog depth > 20 sustained 30m |
| Advanced query optimization | No performance regression | p95 > 11s traced to query plan |
| Multi-device sync conflicts | Single user assumption | Second device request |

---

## 7. Rollout & Telemetry (Local-Only)

### 7.1 Metrics Emission

**Storage:** `./.adhd-brain/.metrics/YYYY-MM-DD.ndjson`

**Activation:** `CAPTURE_METRICS=1` environment variable

**Privacy:** Local-only, never transmitted externally

**Format:**

```json
{"timestamp":"2025-09-27T10:30:00.000Z","name":"capture_staging_ms","value":45.3,"labels":{"source":"voice"}}
{"timestamp":"2025-09-27T10:30:01.000Z","name":"captures_inserted_total","value":1,"labels":{"source":"voice"}}
{"timestamp":"2025-09-27T10:30:05.000Z","name":"dedup_check_ms","value":3.2,"labels":{}}
```

**Core Metrics:**

```typescript
interface StagingMetrics {
  // Performance
  capture_staging_ms: number;              // Insert latency
  dedup_check_ms: number;                  // Hash query latency
  recovery_query_ms: number;               // Startup recovery time
  backup_duration_ms: number;              // Hourly backup latency

  // Throughput
  captures_inserted_total: number;         // Counter
  captures_exported_total: number;         // Counter
  dedup_hits_total: number;                // Counter by layer

  // Queue Health
  transcription_queue_depth: number;       // Gauge (staged count)
  export_queue_depth: number;              // Gauge (transcribed count)

  // Failures
  export_failures_total: number;           // Counter
  transcription_failures_total: number;    // Counter
  placeholder_exports_total: number;       // Counter

  // Backup
  backup_verification_result: string;      // "success" | "failure"
  backup_size_bytes: number;               // Gauge
}
```

**Metrics Rotation:**

- Daily rotation at midnight UTC
- Retain 30 days locally
- No aggregation (raw NDJSON for flexibility)

### 7.2 Error Logging

**Structured Errors (errors_log table):**

```typescript
interface ErrorLog {
  id: string;              // ULID
  capture_id: string | null;
  stage: 'poll' | 'transcribe' | 'export' | 'backup' | 'integrity';
  message: string;
  created_at: Date;
}
```

**Example Usage:**

```typescript
await logError({
  capture_id: captureId,
  stage: 'transcribe',
  message: 'Whisper timeout exceeded' // Per [Resilience Guide](../../guides/guide-resilience-patterns.md)
});
```

**Error Aggregation (Health Command):**

```sql
SELECT stage, COUNT(*) as count
FROM errors_log
WHERE created_at > datetime('now', '-1 day')
GROUP BY stage;
```

### 7.3 Feature Flags

**Environment Variables:**

```bash
# Enable metrics collection
CAPTURE_METRICS=1

# Enable fault injection (Phase 2 testing only)
CAPTURE_FAULT_INJECTION=1
CAPTURE_FAULT_POINT=after_capture_insert

# Enable verbose logging
CAPTURE_DEBUG=1
```

**Flag Usage:**

```typescript
if (process.env.CAPTURE_METRICS === '1') {
  emitMetric({ name: 'capture_staging_ms', value: duration });
}

if (process.env.CAPTURE_DEBUG === '1') {
  console.log('[DEBUG] Capture inserted:', captureId);
}
```

---

## 8. Implementation Notes

### 8.1 Connection Pooling

**Strategy:** Single connection per process (synchronous driver)

```typescript
class StagingLedger {
  private db: Database;

  constructor(vaultPath: string) {
    const dbPath = path.join(vaultPath, '.adhd-brain', 'ledger.sqlite');
    this.db = new Database(dbPath);

    // Initialize PRAGMAs
    this.db.pragma('journal_mode = WAL');
    this.db.pragma('synchronous = NORMAL');
    this.db.pragma('foreign_keys = ON');
    // Database timeout per [Resilience Guide](../../guides/guide-resilience-patterns.md#database-resilience)
    this.db.pragma('busy_timeout = 5000');

    // Run migrations
    this.runMigrations();
  }

  close(): void {
    this.db.close();
  }
}
```

### 8.2 Transaction Helpers

```typescript
async transaction<T>(fn: (tx: Transaction) => Promise<T>): Promise<T> {
  const savepoint = `sp_${Date.now()}`;

  try {
    this.db.exec(`SAVEPOINT ${savepoint}`);
    const result = await fn(this.db);
    this.db.exec(`RELEASE ${savepoint}`);
    return result;
  } catch (error) {
    this.db.exec(`ROLLBACK TO ${savepoint}`);
    throw error;
  }
}
```

### 8.3 Query Builders

```typescript
// Prepared statement cache
private stmtCache = new Map<string, Statement>();

private prepare(sql: string): Statement {
  if (!this.stmtCache.has(sql)) {
    this.stmtCache.set(sql, this.db.prepare(sql));
  }
  return this.stmtCache.get(sql)!;
}

// Example usage
async getCapture(id: string): Promise<Capture | null> {
  const stmt = this.prepare('SELECT * FROM captures WHERE id = ?');
  return stmt.get(id) as Capture | null;
}
```

---

## 9. Nerdy Joke Corner

This tech spec is like SQLite's documentation‚Äîcomprehensive, slightly intimidating, and full of PRAGMAs you'll cargo-cult without fully understanding. But unlike your brain's working memory, this ledger actually persists state across crashes. Also, "better-sqlite3" is a better name than "adhd-brain" because at least it promises what it delivers: synchronous, fast, and no callbacks to forget about. üß†üíæ

---

## Document Version

**Version:** 0.1.0
**Status:** Draft - Ready for Review
**Last Updated:** 2025-09-27

### Alignment Checklist

- [x] Aligned with Master PRD v2.3.0-MPPP
- [x] Aligned with Staging Ledger PRD v1.0.0-MPPP
- [x] Aligned with Staging Ledger Arch Spec v0.1.0
- [x] Complete SQLite schema with indexes
- [x] TDD Applicability Decision (HIGH risk, required)
- [x] All API contracts defined
- [x] State machine validation logic
- [x] Crash recovery flows documented
- [x] Metrics emission specified
- [x] YAGNI boundaries enforced
- [x] Nerdy joke included

### Next Steps

1. Review with architecture team
2. Create [Staging Ledger Test Spec](./spec-staging-test.md)
3. Implement `@adhd-brain/staging-ledger` package
4. Write TDD test suite (unit + integration + contract)
5. Validate with 50+ real captures

---

**End of Technical Specification**